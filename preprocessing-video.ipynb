{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyLvcl6DPTSJwdDpCcTBrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s11khushboo/youtube-QandA/blob/main/preprocessing-video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp openai-whisper sentence-transformers pinecone\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GGmSbW__1Mpn",
        "outputId": "f7b0a71b-7a1d-48d9-8d93-33802803ff4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.11.12)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-8.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.11.12)\n",
            "Requirement already satisfied: orjson>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.11.4)\n",
            "Collecting pinecone-plugin-assistant<4.0.0,>=3.0.1 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.1.0,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Collecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Downloading pinecone-8.0.0-py3-none-any.whl (745 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.9/745.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.9/280.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, pinecone-plugin-assistant, pinecone\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed packaging-24.2 pinecone-8.0.0 pinecone-plugin-assistant-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "05cf114dcba744c88a8cc0cb79bb9949"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ingest.py (simplified)\n",
        "from yt_dlp import YoutubeDL\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pinecone\n",
        "import uuid\n",
        "import math\n",
        "import time\n",
        "\n",
        "INDEX_NAME = \"youtube-chunks\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # or OpenAI embeddings\n",
        "WHISPER_MODEL = \"small\"\n",
        "# download audio\n",
        "def download_audio(youtube_url, out_path=\"audio.mp3\"):\n",
        "    ydl_opts = {\"format\": \"bestaudio/best\", \"outtmpl\": out_path}\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([youtube_url])\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "1aKxmQPe1CKY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transcribe\n",
        "def transcribe_whisper(audio_path):\n",
        "    model = whisper.load_model(WHISPER_MODEL)\n",
        "    result = model.transcribe(audio_path, task=\"transcribe\")  # returns segments with timestamps\n",
        "    return result  # {\"text\": \"...\", \"segments\": [{start,end,text}], ...}"
      ],
      "metadata": {
        "id": "1MN2vOmG_opL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_youtube_video(url):\n",
        "    print(\"Downloading audio...\")\n",
        "    audio_path = download_audio(url)\n",
        "\n",
        "    return audio_path\n",
        "print(ingest_youtube_video(\"https://youtu.be/dwlE7TiDXz4?si=SvVKbWlBuInfYECa\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzotZ26I_031",
        "outputId": "9eaca0d5-7184-492e-bfc8-ae811ba91a82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading audio...\n",
            "[youtube] Extracting URL: https://youtu.be/dwlE7TiDXz4?si=SvVKbWlBuInfYECa\n",
            "[youtube] dwlE7TiDXz4: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] dwlE7TiDXz4: Downloading android sdkless player API JSON\n",
            "[youtube] dwlE7TiDXz4: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] dwlE7TiDXz4: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] dwlE7TiDXz4: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] dwlE7TiDXz4: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] dwlE7TiDXz4: Downloading 1 format(s): 251-12\n",
            "[download] audio.mp3 has already been downloaded\n",
            "[download] 100% of    7.44MiB\n",
            "audio.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Transcribing audio...\")\n",
        "transcript = transcribe_whisper(\"/content/audio.mp3\")\n",
        "print(transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btJXrokqB8pf",
        "outputId": "454ba47b-25db-4e41-f4a2-c5fc3ac64351"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:06<00:00, 77.8MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \" MCP vs API. Will MCP replace API? MCP flips this completely. AI, models and AI agents. APIs are like a restaurant menu. No pre-written code, no manual integration. With APIs, discovery is static. If you asked an AI agent to order you a pizza, book a doctor's appointment, and send an email to your boss, all in one go, could it actually do that? The answer is yes, but not with traditional APIs. It's with MCP, Model Context Protocol. If you're new here, I am Priyanka and on this channel, we break down cloud and AI technologies for developers and tech practitioners who want to stay ahead of the curve. Now today, we're diving into MCP vs API, a debate that is fundamentally changing how you think about AI, agent and development. Let's start with one sentence difference between MCP and APIs. Now APIs are built for human developers to manually integrate software systems. While MCP is specifically designed for AI, models and AI agents to dynamically and autonomously interact with software and tools. Now think of it this way. APIs are like a restaurant menu. You, the human developer, read the menu, understand the options and place a specific order. You get exactly what you asked for. Now MCP is like having a smart assistant at that same restaurant. Instead of telling the assistant exactly which dish to order, you just say, I'm hungry and I need something healthy. The assistant can look at the menu itself, understand what's available and make intelligent decisions about what to order for you. That's the fundamental shift. APIs require humans to write explicit code for every action. MCP allows AI agents to discover and use tools on its own. Now let's go deeper. There are three fundamental differences between APIs and MCP that you need to understand. Now number one difference is the user. APIs are designed for human developers, like I said earlier. When you use an API, you read the documentation. You understand the available endpoints, the data formats, the authentication methods, and then you write explicit code to call a specific endpoint, handle that response and manage errors. The entire workflow is manually coded by you, the human developer. Now MCP flips this completely. With MCP, an AI agent connects to an MCP server and asks, what can you do? The server responds with a manifest of its available resources. That's data, tools, and functions. The AI can then reason about this information and decide which tools to use and in what sequence to complete a specific task. No pre-written code, no manual integration. The AI agent figures it out. Let me show you this with a real example. Let's say you want to get weather data for New York City using a traditional API. You, the developer, would go to the Open Weather API documentation. Read through it to find the right endpoints. Understand that you need latitude and longitude or a city ID. Then write code, you handle the responses, parse the JSON, extract the temperature, and display it. That's five distinct steps you manually coded. Now here's the same task with MCP. An AI agent connects to a weather MCP server and simply asks, what weather data can you provide? The server responds, I can provide current weather, forecasts, and historical data for any location. Just tell me what you need. The agent decides, I need current weather for New York City and the server handles the rest. One autonomous interaction, no documentation reading, no code writing, the AI agent understood the capability and used it. Difference two is interaction. Now API communication is instruction based. It's rigid. You tell the system exactly what to do. For example, if you want user data, you must call get function slash API slash users with a specific user ID. Now you get exactly what you asked for, nothing more, nothing less. MCP interaction is goal based. You tell the AI agent what you want to achieve, not how to do it. Let me give you a real example here. Imagine you task an AI agent with disinformation. Find the contact information for the lead on Project Phoenix account and draft an email to them. With traditional APIs, you would need to write separate code to query the CRM, find the contact information, and use an email API. Three different interactions. With MCP, the AI agent can use one MCP server to autonomously, number one, query a CRM tool to find Project Lead's name. Number two, use a contact directory tool to find their email address. And number three, use an email tool to draft and save this message. Now the AI dynamically chains these tools together. That's a level of autonomy that traditional APIs alone do not facilitate. Now difference three is discovery. With APIs, discovery is static. To use an API, you need documentation. If the API changes, the docs must be updated and you might need to rewrite your code as well. This process is static and external to the API itself. Now MCP servers are self-describing. When an AI agent connects to an MCP server, the server provides a machine-readable menu of its capabilities on the fly. This means an AI agent can always access the most current set of tools without relying on external documentation. Tools can be added, removed, and updated on the server, and the AI agent will immediately understand the new state of affairs. This makes the system more resilient and adaptable. So here's the million dollar question, which you came here to answer. Will MCP replace APIs? The answer is no. MCP will complement APIs, not replace them. Think of it like this. APIs are the foundational plumbing, connecting different services. MCP is the smart universal remote control that an AI agent can use to operate that plumbing. Now here's how they work together. First, many MCP servers will be rappers around existing enterprise APIs. We've got a lot of APIs in every single company. Now companies have invested decades in building robust API infrastructures. These aren't really going away anywhere. MCP provides a standardized way to make these existing assets AI ready. Now second is MCP makes it easy to expose a vast number of diverse tools to an AI agent in a uniform way. Trying to get an LLM to reliably parse thousands of different open API specs would be far less efficient. And third, in AI-driven systems, the AI agent becomes the central orchestrator. MCP is the protocol that allows this asset to discover and command a fleet of tools. While human-driven applications will continue to rely on traditional APIs, agent-driven workflows will increasingly depend on MCPs. So what does this mean for you as a developer or a tech professional or an AI engineer? If you are building traditional software that humans use directly, APIs aren't going away anywhere. You're good. But if you're building anything involving AI agents, and let's be honest, that's becoming pretty much everything these days, you need to understand MCP. The companies adopting MCP early are the ones building AI agents that can actually take action, not just generate text. They are building AI that can autonomously manage workflows, access enterprise data, and execute complex multi-step tasks. And they're doing it faster and more efficiently than anyone trying to manually code API integrations for every single use case. The shift from APIs to MCP isn't just a technical upgrade, it's the difference between building assistance and building AI agents. And if you want to dive deeper into how to actually implement MCP in your own projects, I have created another video breaking down the MCP protocol step by step. Click here to watch that next, and I will see you there.\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 4.5600000000000005, 'text': ' MCP vs API. Will MCP replace API?', 'tokens': [50364, 8797, 47, 12041, 9362, 13, 3099, 8797, 47, 7406, 9362, 30, 50592], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 1, 'seek': 0, 'start': 4.5600000000000005, 'end': 9.26, 'text': ' MCP flips this completely. AI, models and AI agents.', 'tokens': [50592, 8797, 47, 40249, 341, 2584, 13, 7318, 11, 5245, 293, 7318, 12554, 13, 50827], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 2, 'seek': 0, 'start': 9.26, 'end': 11.76, 'text': ' APIs are like a restaurant menu.', 'tokens': [50827, 21445, 366, 411, 257, 6383, 6510, 13, 50952], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 3, 'seek': 0, 'start': 11.76, 'end': 14.96, 'text': ' No pre-written code, no manual integration.', 'tokens': [50952, 883, 659, 12, 26859, 3089, 11, 572, 9688, 10980, 13, 51112], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 4, 'seek': 0, 'start': 14.96, 'end': 17.12, 'text': ' With APIs, discovery is static.', 'tokens': [51112, 2022, 21445, 11, 12114, 307, 13437, 13, 51220], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 5, 'seek': 0, 'start': 18.6, 'end': 21.92, 'text': ' If you asked an AI agent to order you a pizza,', 'tokens': [51294, 759, 291, 2351, 364, 7318, 9461, 281, 1668, 291, 257, 8298, 11, 51460], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 6, 'seek': 0, 'start': 21.92, 'end': 26.240000000000002, 'text': \" book a doctor's appointment, and send an email to your boss,\", 'tokens': [51460, 1446, 257, 4631, 311, 13653, 11, 293, 2845, 364, 3796, 281, 428, 5741, 11, 51676], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 7, 'seek': 0, 'start': 26.240000000000002, 'end': 29.96, 'text': ' all in one go, could it actually do that?', 'tokens': [51676, 439, 294, 472, 352, 11, 727, 309, 767, 360, 300, 30, 51862], 'temperature': 0.0, 'avg_logprob': -0.22723933099543006, 'compression_ratio': 1.48068669527897, 'no_speech_prob': 0.0009226591209881008}, {'id': 8, 'seek': 2996, 'start': 30.0, 'end': 34.36, 'text': ' The answer is yes, but not with traditional APIs.', 'tokens': [50366, 440, 1867, 307, 2086, 11, 457, 406, 365, 5164, 21445, 13, 50584], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 9, 'seek': 2996, 'start': 34.36, 'end': 37.8, 'text': \" It's with MCP, Model Context Protocol.\", 'tokens': [50584, 467, 311, 365, 8797, 47, 11, 17105, 4839, 3828, 48753, 13, 50756], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 10, 'seek': 2996, 'start': 37.8, 'end': 41.120000000000005, 'text': \" If you're new here, I am Priyanka and on this channel,\", 'tokens': [50756, 759, 291, 434, 777, 510, 11, 286, 669, 8087, 88, 21729, 293, 322, 341, 2269, 11, 50922], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 11, 'seek': 2996, 'start': 41.120000000000005, 'end': 45.2, 'text': ' we break down cloud and AI technologies for developers', 'tokens': [50922, 321, 1821, 760, 4588, 293, 7318, 7943, 337, 8849, 51126], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 12, 'seek': 2996, 'start': 45.2, 'end': 49.56, 'text': ' and tech practitioners who want to stay ahead of the curve.', 'tokens': [51126, 293, 7553, 25742, 567, 528, 281, 1754, 2286, 295, 264, 7605, 13, 51344], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 13, 'seek': 2996, 'start': 49.56, 'end': 53.760000000000005, 'text': \" Now today, we're diving into MCP vs API,\", 'tokens': [51344, 823, 965, 11, 321, 434, 20241, 666, 8797, 47, 12041, 9362, 11, 51554], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 14, 'seek': 2996, 'start': 53.760000000000005, 'end': 57.16, 'text': ' a debate that is fundamentally changing', 'tokens': [51554, 257, 7958, 300, 307, 17879, 4473, 51724], 'temperature': 0.0, 'avg_logprob': -0.15701567742132372, 'compression_ratio': 1.4739130434782608, 'no_speech_prob': 0.0017626702319830656}, {'id': 15, 'seek': 5716, 'start': 57.16, 'end': 60.68, 'text': ' how you think about AI, agent and development.', 'tokens': [50364, 577, 291, 519, 466, 7318, 11, 9461, 293, 3250, 13, 50540], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 16, 'seek': 5716, 'start': 60.68, 'end': 63.239999999999995, 'text': \" Let's start with one sentence difference\", 'tokens': [50540, 961, 311, 722, 365, 472, 8174, 2649, 50668], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 17, 'seek': 5716, 'start': 63.239999999999995, 'end': 65.6, 'text': ' between MCP and APIs.', 'tokens': [50668, 1296, 8797, 47, 293, 21445, 13, 50786], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 18, 'seek': 5716, 'start': 65.6, 'end': 69.39999999999999, 'text': ' Now APIs are built for human developers', 'tokens': [50786, 823, 21445, 366, 3094, 337, 1952, 8849, 50976], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 19, 'seek': 5716, 'start': 69.39999999999999, 'end': 73.03999999999999, 'text': ' to manually integrate software systems.', 'tokens': [50976, 281, 16945, 13365, 4722, 3652, 13, 51158], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 20, 'seek': 5716, 'start': 73.03999999999999, 'end': 77.4, 'text': ' While MCP is specifically designed for AI,', 'tokens': [51158, 3987, 8797, 47, 307, 4682, 4761, 337, 7318, 11, 51376], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 21, 'seek': 5716, 'start': 77.4, 'end': 82.4, 'text': ' models and AI agents to dynamically and autonomously', 'tokens': [51376, 5245, 293, 7318, 12554, 281, 43492, 293, 18203, 5098, 51626], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 22, 'seek': 5716, 'start': 82.47999999999999, 'end': 85.16, 'text': ' interact with software and tools.', 'tokens': [51630, 4648, 365, 4722, 293, 3873, 13, 51764], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 23, 'seek': 5716, 'start': 85.16, 'end': 86.32, 'text': ' Now think of it this way.', 'tokens': [51764, 823, 519, 295, 309, 341, 636, 13, 51822], 'temperature': 0.0, 'avg_logprob': -0.13686810416736822, 'compression_ratio': 1.5898617511520738, 'no_speech_prob': 0.003152411663904786}, {'id': 24, 'seek': 8632, 'start': 86.36, 'end': 89.03999999999999, 'text': ' APIs are like a restaurant menu.', 'tokens': [50366, 21445, 366, 411, 257, 6383, 6510, 13, 50500], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 25, 'seek': 8632, 'start': 89.03999999999999, 'end': 92.36, 'text': ' You, the human developer, read the menu,', 'tokens': [50500, 509, 11, 264, 1952, 10754, 11, 1401, 264, 6510, 11, 50666], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 26, 'seek': 8632, 'start': 92.36, 'end': 96.47999999999999, 'text': ' understand the options and place a specific order.', 'tokens': [50666, 1223, 264, 3956, 293, 1081, 257, 2685, 1668, 13, 50872], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 27, 'seek': 8632, 'start': 96.47999999999999, 'end': 99.03999999999999, 'text': ' You get exactly what you asked for.', 'tokens': [50872, 509, 483, 2293, 437, 291, 2351, 337, 13, 51000], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 28, 'seek': 8632, 'start': 99.03999999999999, 'end': 102.16, 'text': ' Now MCP is like having a smart assistant', 'tokens': [51000, 823, 8797, 47, 307, 411, 1419, 257, 4069, 10994, 51156], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 29, 'seek': 8632, 'start': 102.16, 'end': 104.16, 'text': ' at that same restaurant.', 'tokens': [51156, 412, 300, 912, 6383, 13, 51256], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 30, 'seek': 8632, 'start': 104.16, 'end': 106.39999999999999, 'text': ' Instead of telling the assistant exactly', 'tokens': [51256, 7156, 295, 3585, 264, 10994, 2293, 51368], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 31, 'seek': 8632, 'start': 106.39999999999999, 'end': 109.08, 'text': ' which dish to order, you just say,', 'tokens': [51368, 597, 5025, 281, 1668, 11, 291, 445, 584, 11, 51502], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 32, 'seek': 8632, 'start': 109.08, 'end': 112.0, 'text': \" I'm hungry and I need something healthy.\", 'tokens': [51502, 286, 478, 8067, 293, 286, 643, 746, 4627, 13, 51648], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 33, 'seek': 8632, 'start': 112.0, 'end': 115.11999999999999, 'text': ' The assistant can look at the menu itself,', 'tokens': [51648, 440, 10994, 393, 574, 412, 264, 6510, 2564, 11, 51804], 'temperature': 0.0, 'avg_logprob': -0.07967928543831539, 'compression_ratio': 1.6150627615062763, 'no_speech_prob': 0.00022239304962567985}, {'id': 34, 'seek': 11512, 'start': 115.12, 'end': 117.36, 'text': \" understand what's available\", 'tokens': [50364, 1223, 437, 311, 2435, 50476], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 35, 'seek': 11512, 'start': 117.36, 'end': 119.52000000000001, 'text': ' and make intelligent decisions', 'tokens': [50476, 293, 652, 13232, 5327, 50584], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 36, 'seek': 11512, 'start': 119.52000000000001, 'end': 122.56, 'text': ' about what to order for you.', 'tokens': [50584, 466, 437, 281, 1668, 337, 291, 13, 50736], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 37, 'seek': 11512, 'start': 122.56, 'end': 124.72, 'text': \" That's the fundamental shift.\", 'tokens': [50736, 663, 311, 264, 8088, 5513, 13, 50844], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 38, 'seek': 11512, 'start': 124.72, 'end': 128.56, 'text': ' APIs require humans to write explicit code', 'tokens': [50844, 21445, 3651, 6255, 281, 2464, 13691, 3089, 51036], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 39, 'seek': 11512, 'start': 128.56, 'end': 130.56, 'text': ' for every action.', 'tokens': [51036, 337, 633, 3069, 13, 51136], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 40, 'seek': 11512, 'start': 130.56, 'end': 135.56, 'text': ' MCP allows AI agents to discover and use tools on its own.', 'tokens': [51136, 8797, 47, 4045, 7318, 12554, 281, 4411, 293, 764, 3873, 322, 1080, 1065, 13, 51386], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 41, 'seek': 11512, 'start': 136.8, 'end': 138.12, 'text': \" Now let's go deeper.\", 'tokens': [51448, 823, 718, 311, 352, 7731, 13, 51514], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 42, 'seek': 11512, 'start': 138.12, 'end': 140.52, 'text': ' There are three fundamental differences', 'tokens': [51514, 821, 366, 1045, 8088, 7300, 51634], 'temperature': 0.0, 'avg_logprob': -0.07676452785343318, 'compression_ratio': 1.49, 'no_speech_prob': 0.00038766360376030207}, {'id': 43, 'seek': 14052, 'start': 140.52, 'end': 145.36, 'text': ' between APIs and MCP that you need to understand.', 'tokens': [50364, 1296, 21445, 293, 8797, 47, 300, 291, 643, 281, 1223, 13, 50606], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 44, 'seek': 14052, 'start': 145.36, 'end': 148.48000000000002, 'text': ' Now number one difference is the user.', 'tokens': [50606, 823, 1230, 472, 2649, 307, 264, 4195, 13, 50762], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 45, 'seek': 14052, 'start': 148.48000000000002, 'end': 150.84, 'text': ' APIs are designed for human developers,', 'tokens': [50762, 21445, 366, 4761, 337, 1952, 8849, 11, 50880], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 46, 'seek': 14052, 'start': 150.84, 'end': 152.12, 'text': ' like I said earlier.', 'tokens': [50880, 411, 286, 848, 3071, 13, 50944], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 47, 'seek': 14052, 'start': 152.12, 'end': 155.96, 'text': ' When you use an API, you read the documentation.', 'tokens': [50944, 1133, 291, 764, 364, 9362, 11, 291, 1401, 264, 14333, 13, 51136], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 48, 'seek': 14052, 'start': 155.96, 'end': 159.06, 'text': ' You understand the available endpoints,', 'tokens': [51136, 509, 1223, 264, 2435, 917, 20552, 11, 51291], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 49, 'seek': 14052, 'start': 159.06, 'end': 162.36, 'text': ' the data formats, the authentication methods,', 'tokens': [51291, 264, 1412, 25879, 11, 264, 26643, 7150, 11, 51456], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 50, 'seek': 14052, 'start': 162.36, 'end': 165.22, 'text': ' and then you write explicit code', 'tokens': [51456, 293, 550, 291, 2464, 13691, 3089, 51599], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 51, 'seek': 14052, 'start': 165.22, 'end': 167.32000000000002, 'text': ' to call a specific endpoint,', 'tokens': [51599, 281, 818, 257, 2685, 35795, 11, 51704], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 52, 'seek': 14052, 'start': 167.32000000000002, 'end': 170.5, 'text': ' handle that response and manage errors.', 'tokens': [51704, 4813, 300, 4134, 293, 3067, 13603, 13, 51863], 'temperature': 0.0, 'avg_logprob': -0.11918136051722936, 'compression_ratio': 1.5950413223140496, 'no_speech_prob': 0.004256776068359613}, {'id': 53, 'seek': 17050, 'start': 170.5, 'end': 174.26, 'text': ' The entire workflow is manually coded', 'tokens': [50364, 440, 2302, 20993, 307, 16945, 34874, 50552], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 54, 'seek': 17050, 'start': 174.26, 'end': 176.78, 'text': ' by you, the human developer.', 'tokens': [50552, 538, 291, 11, 264, 1952, 10754, 13, 50678], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 55, 'seek': 17050, 'start': 176.78, 'end': 179.26, 'text': ' Now MCP flips this completely.', 'tokens': [50678, 823, 8797, 47, 40249, 341, 2584, 13, 50802], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 56, 'seek': 17050, 'start': 179.26, 'end': 183.46, 'text': ' With MCP, an AI agent connects to an MCP server', 'tokens': [50802, 2022, 8797, 47, 11, 364, 7318, 9461, 16967, 281, 364, 8797, 47, 7154, 51012], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 57, 'seek': 17050, 'start': 183.46, 'end': 186.42, 'text': ' and asks, what can you do?', 'tokens': [51012, 293, 8962, 11, 437, 393, 291, 360, 30, 51160], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 58, 'seek': 17050, 'start': 186.42, 'end': 188.86, 'text': ' The server responds with a manifest', 'tokens': [51160, 440, 7154, 27331, 365, 257, 10067, 51282], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 59, 'seek': 17050, 'start': 188.86, 'end': 191.02, 'text': ' of its available resources.', 'tokens': [51282, 295, 1080, 2435, 3593, 13, 51390], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 60, 'seek': 17050, 'start': 191.02, 'end': 193.94, 'text': \" That's data, tools, and functions.\", 'tokens': [51390, 663, 311, 1412, 11, 3873, 11, 293, 6828, 13, 51536], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 61, 'seek': 17050, 'start': 193.94, 'end': 197.7, 'text': ' The AI can then reason about this information', 'tokens': [51536, 440, 7318, 393, 550, 1778, 466, 341, 1589, 51724], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 62, 'seek': 17050, 'start': 197.7, 'end': 200.22, 'text': ' and decide which tools to use', 'tokens': [51724, 293, 4536, 597, 3873, 281, 764, 51850], 'temperature': 0.0, 'avg_logprob': -0.07917961631853555, 'compression_ratio': 1.5021645021645023, 'no_speech_prob': 0.00011894349154317752}, {'id': 63, 'seek': 20022, 'start': 200.22, 'end': 204.5, 'text': ' and in what sequence to complete a specific task.', 'tokens': [50364, 293, 294, 437, 8310, 281, 3566, 257, 2685, 5633, 13, 50578], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 64, 'seek': 20022, 'start': 204.5, 'end': 208.2, 'text': ' No pre-written code, no manual integration.', 'tokens': [50578, 883, 659, 12, 26859, 3089, 11, 572, 9688, 10980, 13, 50763], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 65, 'seek': 20022, 'start': 208.2, 'end': 210.3, 'text': ' The AI agent figures it out.', 'tokens': [50763, 440, 7318, 9461, 9624, 309, 484, 13, 50868], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 66, 'seek': 20022, 'start': 210.3, 'end': 213.56, 'text': ' Let me show you this with a real example.', 'tokens': [50868, 961, 385, 855, 291, 341, 365, 257, 957, 1365, 13, 51031], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 67, 'seek': 20022, 'start': 213.56, 'end': 216.1, 'text': \" Let's say you want to get weather data\", 'tokens': [51031, 961, 311, 584, 291, 528, 281, 483, 5503, 1412, 51158], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 68, 'seek': 20022, 'start': 216.1, 'end': 219.57999999999998, 'text': ' for New York City using a traditional API.', 'tokens': [51158, 337, 1873, 3609, 4392, 1228, 257, 5164, 9362, 13, 51332], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 69, 'seek': 20022, 'start': 219.57999999999998, 'end': 222.54, 'text': ' You, the developer, would go to', 'tokens': [51332, 509, 11, 264, 10754, 11, 576, 352, 281, 51480], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 70, 'seek': 20022, 'start': 222.54, 'end': 225.66, 'text': ' the Open Weather API documentation.', 'tokens': [51480, 264, 7238, 34441, 9362, 14333, 13, 51636], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 71, 'seek': 20022, 'start': 225.66, 'end': 229.14, 'text': ' Read through it to find the right endpoints.', 'tokens': [51636, 17604, 807, 309, 281, 915, 264, 558, 917, 20552, 13, 51810], 'temperature': 0.0, 'avg_logprob': -0.10487423521099669, 'compression_ratio': 1.477366255144033, 'no_speech_prob': 9.177871106658131e-05}, {'id': 72, 'seek': 22914, 'start': 229.17999999999998, 'end': 232.77999999999997, 'text': ' Understand that you need latitude and longitude', 'tokens': [50366, 26093, 300, 291, 643, 45436, 293, 938, 4377, 50546], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 73, 'seek': 22914, 'start': 232.77999999999997, 'end': 235.01999999999998, 'text': ' or a city ID.', 'tokens': [50546, 420, 257, 2307, 7348, 13, 50658], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 74, 'seek': 22914, 'start': 235.01999999999998, 'end': 238.5, 'text': ' Then write code, you handle the responses,', 'tokens': [50658, 1396, 2464, 3089, 11, 291, 4813, 264, 13019, 11, 50832], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 75, 'seek': 22914, 'start': 238.5, 'end': 243.17999999999998, 'text': ' parse the JSON, extract the temperature, and display it.', 'tokens': [50832, 48377, 264, 31828, 11, 8947, 264, 4292, 11, 293, 4674, 309, 13, 51066], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 76, 'seek': 22914, 'start': 243.17999999999998, 'end': 247.66, 'text': \" That's five distinct steps you manually coded.\", 'tokens': [51066, 663, 311, 1732, 10644, 4439, 291, 16945, 34874, 13, 51290], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 77, 'seek': 22914, 'start': 247.66, 'end': 250.57999999999998, 'text': \" Now here's the same task with MCP.\", 'tokens': [51290, 823, 510, 311, 264, 912, 5633, 365, 8797, 47, 13, 51436], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 78, 'seek': 22914, 'start': 250.57999999999998, 'end': 254.61999999999998, 'text': ' An AI agent connects to a weather MCP server', 'tokens': [51436, 1107, 7318, 9461, 16967, 281, 257, 5503, 8797, 47, 7154, 51638], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 79, 'seek': 22914, 'start': 254.61999999999998, 'end': 258.14, 'text': ' and simply asks, what weather data can you provide?', 'tokens': [51638, 293, 2935, 8962, 11, 437, 5503, 1412, 393, 291, 2893, 30, 51814], 'temperature': 0.0, 'avg_logprob': -0.10822481694428818, 'compression_ratio': 1.497797356828194, 'no_speech_prob': 0.00033645849907770753}, {'id': 80, 'seek': 25814, 'start': 258.14, 'end': 261.53999999999996, 'text': ' The server responds, I can provide current weather,', 'tokens': [50364, 440, 7154, 27331, 11, 286, 393, 2893, 2190, 5503, 11, 50534], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 81, 'seek': 25814, 'start': 261.53999999999996, 'end': 265.14, 'text': ' forecasts, and historical data for any location.', 'tokens': [50534, 49421, 11, 293, 8584, 1412, 337, 604, 4914, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 82, 'seek': 25814, 'start': 265.14, 'end': 266.65999999999997, 'text': ' Just tell me what you need.', 'tokens': [50714, 1449, 980, 385, 437, 291, 643, 13, 50790], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 83, 'seek': 25814, 'start': 266.65999999999997, 'end': 270.58, 'text': ' The agent decides, I need current weather for New York City', 'tokens': [50790, 440, 9461, 14898, 11, 286, 643, 2190, 5503, 337, 1873, 3609, 4392, 50986], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 84, 'seek': 25814, 'start': 270.58, 'end': 272.65999999999997, 'text': ' and the server handles the rest.', 'tokens': [50986, 293, 264, 7154, 18722, 264, 1472, 13, 51090], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 85, 'seek': 25814, 'start': 272.65999999999997, 'end': 277.65999999999997, 'text': ' One autonomous interaction, no documentation reading,', 'tokens': [51090, 1485, 23797, 9285, 11, 572, 14333, 3760, 11, 51340], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 86, 'seek': 25814, 'start': 277.74, 'end': 281.86, 'text': ' no code writing, the AI agent understood', 'tokens': [51344, 572, 3089, 3579, 11, 264, 7318, 9461, 7320, 51550], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 87, 'seek': 25814, 'start': 281.86, 'end': 283.9, 'text': ' the capability and used it.', 'tokens': [51550, 264, 13759, 293, 1143, 309, 13, 51652], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 88, 'seek': 25814, 'start': 283.9, 'end': 285.62, 'text': ' Difference two is interaction.', 'tokens': [51652, 35940, 5158, 732, 307, 9285, 13, 51738], 'temperature': 0.0, 'avg_logprob': -0.09407368526663831, 'compression_ratio': 1.6375545851528384, 'no_speech_prob': 4.336015263106674e-05}, {'id': 89, 'seek': 28562, 'start': 285.62, 'end': 289.02, 'text': ' Now API communication is instruction based.', 'tokens': [50364, 823, 9362, 6101, 307, 10951, 2361, 13, 50534], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 90, 'seek': 28562, 'start': 289.02, 'end': 290.34000000000003, 'text': \" It's rigid.\", 'tokens': [50534, 467, 311, 22195, 13, 50600], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 91, 'seek': 28562, 'start': 290.34000000000003, 'end': 293.06, 'text': ' You tell the system exactly what to do.', 'tokens': [50600, 509, 980, 264, 1185, 2293, 437, 281, 360, 13, 50736], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 92, 'seek': 28562, 'start': 293.06, 'end': 295.94, 'text': ' For example, if you want user data,', 'tokens': [50736, 1171, 1365, 11, 498, 291, 528, 4195, 1412, 11, 50880], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 93, 'seek': 28562, 'start': 295.94, 'end': 300.5, 'text': ' you must call get function slash API slash users', 'tokens': [50880, 291, 1633, 818, 483, 2445, 17330, 9362, 17330, 5022, 51108], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 94, 'seek': 28562, 'start': 300.5, 'end': 302.46, 'text': ' with a specific user ID.', 'tokens': [51108, 365, 257, 2685, 4195, 7348, 13, 51206], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 95, 'seek': 28562, 'start': 302.46, 'end': 304.78000000000003, 'text': ' Now you get exactly what you asked for,', 'tokens': [51206, 823, 291, 483, 2293, 437, 291, 2351, 337, 11, 51322], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 96, 'seek': 28562, 'start': 304.78000000000003, 'end': 306.54, 'text': ' nothing more, nothing less.', 'tokens': [51322, 1825, 544, 11, 1825, 1570, 13, 51410], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 97, 'seek': 28562, 'start': 306.54, 'end': 309.34000000000003, 'text': ' MCP interaction is goal based.', 'tokens': [51410, 8797, 47, 9285, 307, 3387, 2361, 13, 51550], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 98, 'seek': 28562, 'start': 309.34000000000003, 'end': 312.58, 'text': ' You tell the AI agent what you want to achieve,', 'tokens': [51550, 509, 980, 264, 7318, 9461, 437, 291, 528, 281, 4584, 11, 51712], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 99, 'seek': 28562, 'start': 312.58, 'end': 314.26, 'text': ' not how to do it.', 'tokens': [51712, 406, 577, 281, 360, 309, 13, 51796], 'temperature': 0.0, 'avg_logprob': -0.11248653625773493, 'compression_ratio': 1.6371681415929205, 'no_speech_prob': 9.134869469562545e-05}, {'id': 100, 'seek': 31426, 'start': 314.26, 'end': 316.65999999999997, 'text': ' Let me give you a real example here.', 'tokens': [50364, 961, 385, 976, 291, 257, 957, 1365, 510, 13, 50484], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 101, 'seek': 31426, 'start': 316.65999999999997, 'end': 321.34, 'text': ' Imagine you task an AI agent with disinformation.', 'tokens': [50484, 11739, 291, 5633, 364, 7318, 9461, 365, 717, 20941, 13, 50718], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 102, 'seek': 31426, 'start': 321.34, 'end': 324.62, 'text': ' Find the contact information for the lead', 'tokens': [50718, 11809, 264, 3385, 1589, 337, 264, 1477, 50882], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 103, 'seek': 31426, 'start': 324.62, 'end': 328.53999999999996, 'text': ' on Project Phoenix account and draft an email to them.', 'tokens': [50882, 322, 9849, 18383, 2696, 293, 11206, 364, 3796, 281, 552, 13, 51078], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 104, 'seek': 31426, 'start': 328.53999999999996, 'end': 332.21999999999997, 'text': ' With traditional APIs, you would need to write separate code', 'tokens': [51078, 2022, 5164, 21445, 11, 291, 576, 643, 281, 2464, 4994, 3089, 51262], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 105, 'seek': 31426, 'start': 332.21999999999997, 'end': 336.02, 'text': ' to query the CRM, find the contact information,', 'tokens': [51262, 281, 14581, 264, 14123, 44, 11, 915, 264, 3385, 1589, 11, 51452], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 106, 'seek': 31426, 'start': 336.02, 'end': 338.46, 'text': ' and use an email API.', 'tokens': [51452, 293, 764, 364, 3796, 9362, 13, 51574], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 107, 'seek': 31426, 'start': 338.46, 'end': 340.86, 'text': ' Three different interactions.', 'tokens': [51574, 6244, 819, 13280, 13, 51694], 'temperature': 0.0, 'avg_logprob': -0.13787017471488866, 'compression_ratio': 1.5852534562211982, 'no_speech_prob': 0.0005807616398669779}, {'id': 108, 'seek': 34086, 'start': 340.86, 'end': 344.62, 'text': ' With MCP, the AI agent can use one MCP server', 'tokens': [50364, 2022, 8797, 47, 11, 264, 7318, 9461, 393, 764, 472, 8797, 47, 7154, 50552], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 109, 'seek': 34086, 'start': 344.62, 'end': 348.74, 'text': ' to autonomously, number one, query a CRM tool', 'tokens': [50552, 281, 18203, 5098, 11, 1230, 472, 11, 14581, 257, 14123, 44, 2290, 50758], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 110, 'seek': 34086, 'start': 348.74, 'end': 350.74, 'text': \" to find Project Lead's name.\", 'tokens': [50758, 281, 915, 9849, 31025, 311, 1315, 13, 50858], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 111, 'seek': 34086, 'start': 350.74, 'end': 353.42, 'text': ' Number two, use a contact directory tool', 'tokens': [50858, 5118, 732, 11, 764, 257, 3385, 21120, 2290, 50992], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 112, 'seek': 34086, 'start': 353.42, 'end': 355.18, 'text': ' to find their email address.', 'tokens': [50992, 281, 915, 641, 3796, 2985, 13, 51080], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 113, 'seek': 34086, 'start': 355.18, 'end': 357.82, 'text': ' And number three, use an email tool', 'tokens': [51080, 400, 1230, 1045, 11, 764, 364, 3796, 2290, 51212], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 114, 'seek': 34086, 'start': 357.82, 'end': 360.06, 'text': ' to draft and save this message.', 'tokens': [51212, 281, 11206, 293, 3155, 341, 3636, 13, 51324], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 115, 'seek': 34086, 'start': 360.06, 'end': 364.26, 'text': ' Now the AI dynamically chains these tools together.', 'tokens': [51324, 823, 264, 7318, 43492, 12626, 613, 3873, 1214, 13, 51534], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 116, 'seek': 34086, 'start': 364.26, 'end': 366.06, 'text': \" That's a level of autonomy\", 'tokens': [51534, 663, 311, 257, 1496, 295, 27278, 51624], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 117, 'seek': 34086, 'start': 366.06, 'end': 369.5, 'text': ' that traditional APIs alone do not facilitate.', 'tokens': [51624, 300, 5164, 21445, 3312, 360, 406, 20207, 13, 51796], 'temperature': 0.0, 'avg_logprob': -0.08165109382485444, 'compression_ratio': 1.5867768595041323, 'no_speech_prob': 0.002226763404905796}, {'id': 118, 'seek': 36950, 'start': 369.5, 'end': 371.7, 'text': ' Now difference three is discovery.', 'tokens': [50364, 823, 2649, 1045, 307, 12114, 13, 50474], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 119, 'seek': 36950, 'start': 371.7, 'end': 374.38, 'text': ' With APIs, discovery is static.', 'tokens': [50474, 2022, 21445, 11, 12114, 307, 13437, 13, 50608], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 120, 'seek': 36950, 'start': 374.38, 'end': 377.02, 'text': ' To use an API, you need documentation.', 'tokens': [50608, 1407, 764, 364, 9362, 11, 291, 643, 14333, 13, 50740], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 121, 'seek': 36950, 'start': 377.02, 'end': 380.42, 'text': ' If the API changes, the docs must be updated', 'tokens': [50740, 759, 264, 9362, 2962, 11, 264, 45623, 1633, 312, 10588, 50910], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 122, 'seek': 36950, 'start': 380.42, 'end': 383.46, 'text': ' and you might need to rewrite your code as well.', 'tokens': [50910, 293, 291, 1062, 643, 281, 28132, 428, 3089, 382, 731, 13, 51062], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 123, 'seek': 36950, 'start': 383.46, 'end': 387.7, 'text': ' This process is static and external to the API itself.', 'tokens': [51062, 639, 1399, 307, 13437, 293, 8320, 281, 264, 9362, 2564, 13, 51274], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 124, 'seek': 36950, 'start': 387.7, 'end': 391.1, 'text': ' Now MCP servers are self-describing.', 'tokens': [51274, 823, 8797, 47, 15909, 366, 2698, 12, 14792, 39541, 13, 51444], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 125, 'seek': 36950, 'start': 391.1, 'end': 394.18, 'text': ' When an AI agent connects to an MCP server,', 'tokens': [51444, 1133, 364, 7318, 9461, 16967, 281, 364, 8797, 47, 7154, 11, 51598], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 126, 'seek': 36950, 'start': 394.18, 'end': 397.58, 'text': ' the server provides a machine-readable menu', 'tokens': [51598, 264, 7154, 6417, 257, 3479, 12, 2538, 712, 6510, 51768], 'temperature': 0.0, 'avg_logprob': -0.06674074209653415, 'compression_ratio': 1.6059322033898304, 'no_speech_prob': 0.00041794541175477207}, {'id': 127, 'seek': 39758, 'start': 397.58, 'end': 400.53999999999996, 'text': ' of its capabilities on the fly.', 'tokens': [50364, 295, 1080, 10862, 322, 264, 3603, 13, 50512], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 128, 'seek': 39758, 'start': 400.53999999999996, 'end': 403.41999999999996, 'text': ' This means an AI agent can always access', 'tokens': [50512, 639, 1355, 364, 7318, 9461, 393, 1009, 2105, 50656], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 129, 'seek': 39758, 'start': 403.41999999999996, 'end': 405.5, 'text': ' the most current set of tools', 'tokens': [50656, 264, 881, 2190, 992, 295, 3873, 50760], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 130, 'seek': 39758, 'start': 405.5, 'end': 408.65999999999997, 'text': ' without relying on external documentation.', 'tokens': [50760, 1553, 24140, 322, 8320, 14333, 13, 50918], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 131, 'seek': 39758, 'start': 408.65999999999997, 'end': 412.41999999999996, 'text': ' Tools can be added, removed, and updated on the server,', 'tokens': [50918, 30302, 393, 312, 3869, 11, 7261, 11, 293, 10588, 322, 264, 7154, 11, 51106], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 132, 'seek': 39758, 'start': 412.41999999999996, 'end': 415.58, 'text': ' and the AI agent will immediately understand', 'tokens': [51106, 293, 264, 7318, 9461, 486, 4258, 1223, 51264], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 133, 'seek': 39758, 'start': 415.58, 'end': 417.62, 'text': ' the new state of affairs.', 'tokens': [51264, 264, 777, 1785, 295, 17478, 13, 51366], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 134, 'seek': 39758, 'start': 417.62, 'end': 421.41999999999996, 'text': ' This makes the system more resilient and adaptable.', 'tokens': [51366, 639, 1669, 264, 1185, 544, 23699, 293, 6231, 712, 13, 51556], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 135, 'seek': 39758, 'start': 421.41999999999996, 'end': 424.14, 'text': \" So here's the million dollar question,\", 'tokens': [51556, 407, 510, 311, 264, 2459, 7241, 1168, 11, 51692], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 136, 'seek': 39758, 'start': 424.14, 'end': 426.62, 'text': ' which you came here to answer.', 'tokens': [51692, 597, 291, 1361, 510, 281, 1867, 13, 51816], 'temperature': 0.0, 'avg_logprob': -0.10004186630249023, 'compression_ratio': 1.6213991769547325, 'no_speech_prob': 0.0015540338354185224}, {'id': 137, 'seek': 42662, 'start': 426.66, 'end': 428.9, 'text': ' Will MCP replace APIs?', 'tokens': [50366, 3099, 8797, 47, 7406, 21445, 30, 50478], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 138, 'seek': 42662, 'start': 428.9, 'end': 430.9, 'text': ' The answer is no.', 'tokens': [50478, 440, 1867, 307, 572, 13, 50578], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 139, 'seek': 42662, 'start': 430.9, 'end': 435.46, 'text': ' MCP will complement APIs, not replace them.', 'tokens': [50578, 8797, 47, 486, 17103, 21445, 11, 406, 7406, 552, 13, 50806], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 140, 'seek': 42662, 'start': 435.46, 'end': 436.86, 'text': ' Think of it like this.', 'tokens': [50806, 6557, 295, 309, 411, 341, 13, 50876], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 141, 'seek': 42662, 'start': 436.86, 'end': 440.22, 'text': ' APIs are the foundational plumbing,', 'tokens': [50876, 21445, 366, 264, 32195, 39993, 11, 51044], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 142, 'seek': 42662, 'start': 440.22, 'end': 442.54, 'text': ' connecting different services.', 'tokens': [51044, 11015, 819, 3328, 13, 51160], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 143, 'seek': 42662, 'start': 442.54, 'end': 446.74, 'text': ' MCP is the smart universal remote control', 'tokens': [51160, 8797, 47, 307, 264, 4069, 11455, 8607, 1969, 51370], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 144, 'seek': 42662, 'start': 446.74, 'end': 451.26, 'text': ' that an AI agent can use to operate that plumbing.', 'tokens': [51370, 300, 364, 7318, 9461, 393, 764, 281, 9651, 300, 39993, 13, 51596], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 145, 'seek': 42662, 'start': 451.26, 'end': 452.94, 'text': \" Now here's how they work together.\", 'tokens': [51596, 823, 510, 311, 577, 436, 589, 1214, 13, 51680], 'temperature': 0.0, 'avg_logprob': -0.10002225921267555, 'compression_ratio': 1.4803921568627452, 'no_speech_prob': 0.0034403889440000057}, {'id': 146, 'seek': 45294, 'start': 452.94, 'end': 456.9, 'text': ' First, many MCP servers will be rappers', 'tokens': [50364, 2386, 11, 867, 8797, 47, 15909, 486, 312, 45025, 50562], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 147, 'seek': 45294, 'start': 456.9, 'end': 459.62, 'text': ' around existing enterprise APIs.', 'tokens': [50562, 926, 6741, 14132, 21445, 13, 50698], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 148, 'seek': 45294, 'start': 459.62, 'end': 463.21999999999997, 'text': \" We've got a lot of APIs in every single company.\", 'tokens': [50698, 492, 600, 658, 257, 688, 295, 21445, 294, 633, 2167, 2237, 13, 50878], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 149, 'seek': 45294, 'start': 463.21999999999997, 'end': 465.3, 'text': ' Now companies have invested decades', 'tokens': [50878, 823, 3431, 362, 13104, 7878, 50982], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 150, 'seek': 45294, 'start': 465.3, 'end': 468.5, 'text': ' in building robust API infrastructures.', 'tokens': [50982, 294, 2390, 13956, 9362, 6534, 44513, 13, 51142], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 151, 'seek': 45294, 'start': 468.5, 'end': 470.82, 'text': \" These aren't really going away anywhere.\", 'tokens': [51142, 1981, 3212, 380, 534, 516, 1314, 4992, 13, 51258], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 152, 'seek': 45294, 'start': 470.82, 'end': 473.9, 'text': ' MCP provides a standardized way', 'tokens': [51258, 8797, 47, 6417, 257, 31677, 636, 51412], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 153, 'seek': 45294, 'start': 473.9, 'end': 477.74, 'text': ' to make these existing assets AI ready.', 'tokens': [51412, 281, 652, 613, 6741, 9769, 7318, 1919, 13, 51604], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 154, 'seek': 45294, 'start': 477.74, 'end': 480.9, 'text': ' Now second is MCP makes it easy to expose', 'tokens': [51604, 823, 1150, 307, 8797, 47, 1669, 309, 1858, 281, 19219, 51762], 'temperature': 0.0, 'avg_logprob': -0.09271362092759874, 'compression_ratio': 1.5107296137339057, 'no_speech_prob': 0.0014315758598968387}, {'id': 155, 'seek': 48090, 'start': 480.9, 'end': 484.65999999999997, 'text': ' a vast number of diverse tools to an AI agent', 'tokens': [50364, 257, 8369, 1230, 295, 9521, 3873, 281, 364, 7318, 9461, 50552], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 156, 'seek': 48090, 'start': 484.65999999999997, 'end': 486.21999999999997, 'text': ' in a uniform way.', 'tokens': [50552, 294, 257, 9452, 636, 13, 50630], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 157, 'seek': 48090, 'start': 486.21999999999997, 'end': 488.85999999999996, 'text': ' Trying to get an LLM to reliably parse', 'tokens': [50630, 20180, 281, 483, 364, 441, 43, 44, 281, 49927, 48377, 50762], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 158, 'seek': 48090, 'start': 488.85999999999996, 'end': 491.53999999999996, 'text': ' thousands of different open API specs', 'tokens': [50762, 5383, 295, 819, 1269, 9362, 27911, 50896], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 159, 'seek': 48090, 'start': 491.53999999999996, 'end': 493.62, 'text': ' would be far less efficient.', 'tokens': [50896, 576, 312, 1400, 1570, 7148, 13, 51000], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 160, 'seek': 48090, 'start': 493.62, 'end': 496.26, 'text': ' And third, in AI-driven systems,', 'tokens': [51000, 400, 2636, 11, 294, 7318, 12, 25456, 3652, 11, 51132], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 161, 'seek': 48090, 'start': 496.26, 'end': 500.02, 'text': ' the AI agent becomes the central orchestrator.', 'tokens': [51132, 264, 7318, 9461, 3643, 264, 5777, 14161, 19802, 13, 51320], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 162, 'seek': 48090, 'start': 500.02, 'end': 503.41999999999996, 'text': ' MCP is the protocol that allows this asset', 'tokens': [51320, 8797, 47, 307, 264, 10336, 300, 4045, 341, 11999, 51490], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 163, 'seek': 48090, 'start': 503.41999999999996, 'end': 507.09999999999997, 'text': ' to discover and command a fleet of tools.', 'tokens': [51490, 281, 4411, 293, 5622, 257, 19396, 295, 3873, 13, 51674], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 164, 'seek': 48090, 'start': 507.09999999999997, 'end': 509.5, 'text': ' While human-driven applications', 'tokens': [51674, 3987, 1952, 12, 25456, 5821, 51794], 'temperature': 0.0, 'avg_logprob': -0.08808906555175781, 'compression_ratio': 1.525, 'no_speech_prob': 0.0009002312435768545}, {'id': 165, 'seek': 50950, 'start': 509.5, 'end': 513.14, 'text': ' will continue to rely on traditional APIs,', 'tokens': [50364, 486, 2354, 281, 10687, 322, 5164, 21445, 11, 50546], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 166, 'seek': 50950, 'start': 513.14, 'end': 517.86, 'text': ' agent-driven workflows will increasingly depend on MCPs.', 'tokens': [50546, 9461, 12, 25456, 43461, 486, 12980, 5672, 322, 8797, 23043, 13, 50782], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 167, 'seek': 50950, 'start': 517.86, 'end': 521.5, 'text': ' So what does this mean for you as a developer', 'tokens': [50782, 407, 437, 775, 341, 914, 337, 291, 382, 257, 10754, 50964], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 168, 'seek': 50950, 'start': 521.5, 'end': 524.9, 'text': ' or a tech professional or an AI engineer?', 'tokens': [50964, 420, 257, 7553, 4843, 420, 364, 7318, 11403, 30, 51134], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 169, 'seek': 50950, 'start': 524.9, 'end': 527.66, 'text': ' If you are building traditional software', 'tokens': [51134, 759, 291, 366, 2390, 5164, 4722, 51272], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 170, 'seek': 50950, 'start': 527.66, 'end': 529.9, 'text': ' that humans use directly,', 'tokens': [51272, 300, 6255, 764, 3838, 11, 51384], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 171, 'seek': 50950, 'start': 529.9, 'end': 532.42, 'text': \" APIs aren't going away anywhere.\", 'tokens': [51384, 21445, 3212, 380, 516, 1314, 4992, 13, 51510], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 172, 'seek': 50950, 'start': 532.42, 'end': 533.26, 'text': \" You're good.\", 'tokens': [51510, 509, 434, 665, 13, 51552], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 173, 'seek': 50950, 'start': 533.26, 'end': 537.38, 'text': \" But if you're building anything involving AI agents,\", 'tokens': [51552, 583, 498, 291, 434, 2390, 1340, 17030, 7318, 12554, 11, 51758], 'temperature': 0.0, 'avg_logprob': -0.08679918183220757, 'compression_ratio': 1.5150214592274678, 'no_speech_prob': 0.0038575921207666397}, {'id': 174, 'seek': 53738, 'start': 537.38, 'end': 538.62, 'text': \" and let's be honest,\", 'tokens': [50364, 293, 718, 311, 312, 3245, 11, 50426], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 175, 'seek': 53738, 'start': 538.62, 'end': 541.5, 'text': \" that's becoming pretty much everything these days,\", 'tokens': [50426, 300, 311, 5617, 1238, 709, 1203, 613, 1708, 11, 50570], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 176, 'seek': 53738, 'start': 541.5, 'end': 544.46, 'text': ' you need to understand MCP.', 'tokens': [50570, 291, 643, 281, 1223, 8797, 47, 13, 50718], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 177, 'seek': 53738, 'start': 544.46, 'end': 547.02, 'text': ' The companies adopting MCP early', 'tokens': [50718, 440, 3431, 32328, 8797, 47, 2440, 50846], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 178, 'seek': 53738, 'start': 547.02, 'end': 549.34, 'text': ' are the ones building AI agents', 'tokens': [50846, 366, 264, 2306, 2390, 7318, 12554, 50962], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 179, 'seek': 53738, 'start': 549.34, 'end': 553.7, 'text': ' that can actually take action, not just generate text.', 'tokens': [50962, 300, 393, 767, 747, 3069, 11, 406, 445, 8460, 2487, 13, 51180], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 180, 'seek': 53738, 'start': 553.7, 'end': 558.02, 'text': ' They are building AI that can autonomously manage workflows,', 'tokens': [51180, 814, 366, 2390, 7318, 300, 393, 18203, 5098, 3067, 43461, 11, 51396], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 181, 'seek': 53738, 'start': 558.02, 'end': 559.9, 'text': ' access enterprise data,', 'tokens': [51396, 2105, 14132, 1412, 11, 51490], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 182, 'seek': 53738, 'start': 559.9, 'end': 564.1, 'text': ' and execute complex multi-step tasks.', 'tokens': [51490, 293, 14483, 3997, 4825, 12, 16792, 9608, 13, 51700], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 183, 'seek': 53738, 'start': 564.1, 'end': 567.36, 'text': \" And they're doing it faster and more efficiently\", 'tokens': [51700, 400, 436, 434, 884, 309, 4663, 293, 544, 19621, 51863], 'temperature': 0.0, 'avg_logprob': -0.09966282892708826, 'compression_ratio': 1.5766129032258065, 'no_speech_prob': 0.0013184880372136831}, {'id': 184, 'seek': 56736, 'start': 567.36, 'end': 571.92, 'text': ' than anyone trying to manually code API integrations', 'tokens': [50364, 813, 2878, 1382, 281, 16945, 3089, 9362, 3572, 763, 50592], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 185, 'seek': 56736, 'start': 571.92, 'end': 574.32, 'text': ' for every single use case.', 'tokens': [50592, 337, 633, 2167, 764, 1389, 13, 50712], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 186, 'seek': 56736, 'start': 574.32, 'end': 576.44, 'text': ' The shift from APIs to MCP', 'tokens': [50712, 440, 5513, 490, 21445, 281, 8797, 47, 50818], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 187, 'seek': 56736, 'start': 576.44, 'end': 579.08, 'text': \" isn't just a technical upgrade,\", 'tokens': [50818, 1943, 380, 445, 257, 6191, 11484, 11, 50950], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 188, 'seek': 56736, 'start': 579.08, 'end': 582.36, 'text': \" it's the difference between building assistance\", 'tokens': [50950, 309, 311, 264, 2649, 1296, 2390, 9683, 51114], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 189, 'seek': 56736, 'start': 582.36, 'end': 584.36, 'text': ' and building AI agents.', 'tokens': [51114, 293, 2390, 7318, 12554, 13, 51214], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 190, 'seek': 56736, 'start': 584.36, 'end': 586.16, 'text': ' And if you want to dive deeper', 'tokens': [51214, 400, 498, 291, 528, 281, 9192, 7731, 51304], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 191, 'seek': 56736, 'start': 586.16, 'end': 591.16, 'text': ' into how to actually implement MCP in your own projects,', 'tokens': [51304, 666, 577, 281, 767, 4445, 8797, 47, 294, 428, 1065, 4455, 11, 51554], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 192, 'seek': 56736, 'start': 591.8000000000001, 'end': 594.28, 'text': ' I have created another video', 'tokens': [51586, 286, 362, 2942, 1071, 960, 51710], 'temperature': 0.0, 'avg_logprob': -0.09691303477567785, 'compression_ratio': 1.4863636363636363, 'no_speech_prob': 0.000325725763104856}, {'id': 193, 'seek': 59428, 'start': 594.28, 'end': 598.56, 'text': ' breaking down the MCP protocol step by step.', 'tokens': [50364, 7697, 760, 264, 8797, 47, 10336, 1823, 538, 1823, 13, 50578], 'temperature': 0.0, 'avg_logprob': -0.1878276486550608, 'compression_ratio': 1.086021505376344, 'no_speech_prob': 0.06403128802776337}, {'id': 194, 'seek': 59428, 'start': 598.56, 'end': 600.4399999999999, 'text': ' Click here to watch that next,', 'tokens': [50578, 8230, 510, 281, 1159, 300, 958, 11, 50672], 'temperature': 0.0, 'avg_logprob': -0.1878276486550608, 'compression_ratio': 1.086021505376344, 'no_speech_prob': 0.06403128802776337}, {'id': 195, 'seek': 59428, 'start': 600.4399999999999, 'end': 602.24, 'text': ' and I will see you there.', 'tokens': [50672, 293, 286, 486, 536, 291, 456, 13, 50762], 'temperature': 0.0, 'avg_logprob': -0.1878276486550608, 'compression_ratio': 1.086021505376344, 'no_speech_prob': 0.06403128802776337}], 'language': 'en'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# chunking with overlap\n",
        "def chunk_segments(segments, max_chars=1000, overlap_chars=200):\n",
        "    chunks = []\n",
        "    buffer = \"\"\n",
        "    buffer_start = None\n",
        "    buffer_end = None\n",
        "    for seg in segments:\n",
        "        text = seg[\"text\"].strip()\n",
        "        if not buffer:\n",
        "            buffer_start = seg[\"start\"]\n",
        "        if len(buffer) + len(text) <= max_chars:\n",
        "            buffer += (\" \" + text)\n",
        "            buffer_end = seg[\"end\"]\n",
        "        else:\n",
        "            chunks.append({\n",
        "                \"start\": buffer_start, \"end\": buffer_end, \"text\": buffer.strip()\n",
        "            })\n",
        "            # start new buffer with overlap\n",
        "            buffer = text[-overlap_chars:]\n",
        "            buffer_start = seg[\"start\"]\n",
        "            buffer_end = seg[\"end\"]\n",
        "    if buffer:\n",
        "        chunks.append({\"start\": buffer_start, \"end\": buffer_end, \"text\": buffer.strip()})\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "9EqYKKWhE752"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=chunk_segments(transcript[\"segments\"])\n",
        "print(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXq7HxL2FHkA",
        "outputId": "a2c6b932-f7c2-4195-f4ec-02093e2b11ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'start': 0.0, 'end': 82.4, 'text': \"MCP vs API. Will MCP replace API? MCP flips this completely. AI, models and AI agents. APIs are like a restaurant menu. No pre-written code, no manual integration. With APIs, discovery is static. If you asked an AI agent to order you a pizza, book a doctor's appointment, and send an email to your boss, all in one go, could it actually do that? The answer is yes, but not with traditional APIs. It's with MCP, Model Context Protocol. If you're new here, I am Priyanka and on this channel, we break down cloud and AI technologies for developers and tech practitioners who want to stay ahead of the curve. Now today, we're diving into MCP vs API, a debate that is fundamentally changing how you think about AI, agent and development. Let's start with one sentence difference between MCP and APIs. Now APIs are built for human developers to manually integrate software systems. While MCP is specifically designed for AI, models and AI agents to dynamically and autonomously\"}, {'start': 82.47999999999999, 'end': 159.06, 'text': \"interact with software and tools. Now think of it this way. APIs are like a restaurant menu. You, the human developer, read the menu, understand the options and place a specific order. You get exactly what you asked for. Now MCP is like having a smart assistant at that same restaurant. Instead of telling the assistant exactly which dish to order, you just say, I'm hungry and I need something healthy. The assistant can look at the menu itself, understand what's available and make intelligent decisions about what to order for you. That's the fundamental shift. APIs require humans to write explicit code for every action. MCP allows AI agents to discover and use tools on its own. Now let's go deeper. There are three fundamental differences between APIs and MCP that you need to understand. Now number one difference is the user. APIs are designed for human developers, like I said earlier. When you use an API, you read the documentation. You understand the available endpoints,\"}, {'start': 159.06, 'end': 238.5, 'text': \"the data formats, the authentication methods, and then you write explicit code to call a specific endpoint, handle that response and manage errors. The entire workflow is manually coded by you, the human developer. Now MCP flips this completely. With MCP, an AI agent connects to an MCP server and asks, what can you do? The server responds with a manifest of its available resources. That's data, tools, and functions. The AI can then reason about this information and decide which tools to use and in what sequence to complete a specific task. No pre-written code, no manual integration. The AI agent figures it out. Let me show you this with a real example. Let's say you want to get weather data for New York City using a traditional API. You, the developer, would go to the Open Weather API documentation. Read through it to find the right endpoints. Understand that you need latitude and longitude or a city ID. Then write code, you handle the responses,\"}, {'start': 238.5, 'end': 314.26, 'text': \"parse the JSON, extract the temperature, and display it. That's five distinct steps you manually coded. Now here's the same task with MCP. An AI agent connects to a weather MCP server and simply asks, what weather data can you provide? The server responds, I can provide current weather, forecasts, and historical data for any location. Just tell me what you need. The agent decides, I need current weather for New York City and the server handles the rest. One autonomous interaction, no documentation reading, no code writing, the AI agent understood the capability and used it. Difference two is interaction. Now API communication is instruction based. It's rigid. You tell the system exactly what to do. For example, if you want user data, you must call get function slash API slash users with a specific user ID. Now you get exactly what you asked for, nothing more, nothing less. MCP interaction is goal based. You tell the AI agent what you want to achieve, not how to do it.\"}, {'start': 314.26, 'end': 387.7, 'text': \"Let me give you a real example here. Imagine you task an AI agent with disinformation. Find the contact information for the lead on Project Phoenix account and draft an email to them. With traditional APIs, you would need to write separate code to query the CRM, find the contact information, and use an email API. Three different interactions. With MCP, the AI agent can use one MCP server to autonomously, number one, query a CRM tool to find Project Lead's name. Number two, use a contact directory tool to find their email address. And number three, use an email tool to draft and save this message. Now the AI dynamically chains these tools together. That's a level of autonomy that traditional APIs alone do not facilitate. Now difference three is discovery. With APIs, discovery is static. To use an API, you need documentation. If the API changes, the docs must be updated and you might need to rewrite your code as well. This process is static and external to the API itself.\"}, {'start': 387.7, 'end': 465.3, 'text': \"Now MCP servers are self-describing. When an AI agent connects to an MCP server, the server provides a machine-readable menu of its capabilities on the fly. This means an AI agent can always access the most current set of tools without relying on external documentation. Tools can be added, removed, and updated on the server, and the AI agent will immediately understand the new state of affairs. This makes the system more resilient and adaptable. So here's the million dollar question, which you came here to answer. Will MCP replace APIs? The answer is no. MCP will complement APIs, not replace them. Think of it like this. APIs are the foundational plumbing, connecting different services. MCP is the smart universal remote control that an AI agent can use to operate that plumbing. Now here's how they work together. First, many MCP servers will be rappers around existing enterprise APIs. We've got a lot of APIs in every single company. Now companies have invested decades\"}, {'start': 465.3, 'end': 541.5, 'text': \"in building robust API infrastructures. These aren't really going away anywhere. MCP provides a standardized way to make these existing assets AI ready. Now second is MCP makes it easy to expose a vast number of diverse tools to an AI agent in a uniform way. Trying to get an LLM to reliably parse thousands of different open API specs would be far less efficient. And third, in AI-driven systems, the AI agent becomes the central orchestrator. MCP is the protocol that allows this asset to discover and command a fleet of tools. While human-driven applications will continue to rely on traditional APIs, agent-driven workflows will increasingly depend on MCPs. So what does this mean for you as a developer or a tech professional or an AI engineer? If you are building traditional software that humans use directly, APIs aren't going away anywhere. You're good. But if you're building anything involving AI agents, and let's be honest, that's becoming pretty much everything these days,\"}, {'start': 541.5, 'end': 602.24, 'text': \"you need to understand MCP. The companies adopting MCP early are the ones building AI agents that can actually take action, not just generate text. They are building AI that can autonomously manage workflows, access enterprise data, and execute complex multi-step tasks. And they're doing it faster and more efficiently than anyone trying to manually code API integrations for every single use case. The shift from APIs to MCP isn't just a technical upgrade, it's the difference between building assistance and building AI agents. And if you want to dive deeper into how to actually implement MCP in your own projects, I have created another video breaking down the MCP protocol step by step. Click here to watch that next, and I will see you there.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdftGof3HDED",
        "outputId": "67d0528b-1f1b-42ca-bedf-f753ef2d9e36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "print(PINECONE_API_KEY)\n",
        "\n",
        "pc = Pinecone(api_key=\"pcsk_6BUe3k_5E2SpCom69vAzzjDpMifRw7dzkkSeXM3Fw2jKeu9J46bgJ1Vrt3fbaXEDjmp4uY\")\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FjQCQj2G4Pw",
        "outputId": "f4edefaa-d73f-43c5-d9e3-91ab422735a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dMC3vTW0mLL",
        "outputId": "3064835c-ee3b-4a91-f904-ccdbbb705d53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
              "                                    'content-length': '150',\n",
              "                                    'content-type': 'application/json',\n",
              "                                    'date': 'Tue, 25 Nov 2025 13:23:19 GMT',\n",
              "                                    'grpc-status': '0',\n",
              "                                    'server': 'envoy',\n",
              "                                    'x-envoy-upstream-service-time': '57',\n",
              "                                    'x-pinecone-request-id': '337189916962666050',\n",
              "                                    'x-pinecone-request-latency-ms': '56'}},\n",
              " 'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'memoryFullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {},\n",
              " 'storageFullness': 0.0,\n",
              " 'total_vector_count': 0,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "\n",
        "\n",
        "# # Config\n",
        "# PINECONE_API_KEY = \"...\"\n",
        "# PINECONE_ENV = \"...\"\n",
        "\n",
        "\n",
        "\n",
        "# embeddings (sentence-transformers)\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "def embed_texts(texts):\n",
        "    return embedder.encode(texts, show_progress_bar=False).tolist()\n",
        "\n",
        "# pinecone init & upsert\n",
        "# pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
        "index_name = \"youtube-text-demo\"\n",
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=384,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def upsert_chunks(video_id, title, chunks):\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embeddings = embed_texts(texts)\n",
        "    items = []\n",
        "    for i,(chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
        "        item = {\n",
        "            \"id\": f\"{video_id}_chunk_{i}\",\n",
        "            \"metadata\": {\n",
        "                \"video_id\": video_id,\n",
        "                \"start_time\": chunk[\"start\"],\n",
        "                \"end_time\": chunk[\"end\"],\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"title\": title\n",
        "            },\n",
        "            \"values\": emb\n",
        "        }\n",
        "        items.append(item)\n",
        "    # upsert in batches\n",
        "    index.upsert(vectors=[(it[\"id\"], it[\"values\"], it[\"metadata\"]) for it in items])"
      ],
      "metadata": {
        "id": "hgpyWyI2J0jj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"Uploading to Pinecone...\")\n",
        "    upsert_chunks(\"dwlE7TiDXz40\",\"MCP vs API: What Every Developer Needs to Know\",chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eobgtlrAMEQH",
        "outputId": "db58bbba-64e5-43a3-db9a-98fdc037248c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading to Pinecone...\n"
          ]
        }
      ]
    }
  ]
}