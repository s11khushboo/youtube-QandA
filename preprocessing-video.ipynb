{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s11khushboo/youtube-QandA/blob/main/preprocessing-video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp openai-whisper sentence-transformers pinecone\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGmSbW__1Mpn",
        "outputId": "54c60610-9f0a-48f1-fbd8-b9b6afe7ed50",
        "collapsed": true
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.11.12)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.12/dist-packages (8.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.11.12)\n",
            "Requirement already satisfied: orjson>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.11.4)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.0.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.1.0,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuQ4d3cwiNz8",
        "outputId": "0d50a34b-1ff1-433a-abfa-cfbae9f6ce14"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.1.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.8.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ingest.py (simplified)\n",
        "from yt_dlp import YoutubeDL\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pinecone\n",
        "import uuid\n",
        "import math\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate  # pseudo imports\n",
        "\n",
        "\n",
        "\n",
        "INDEX_NAME = \"youtube-chunks\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # or OpenAI embeddings\n",
        "WHISPER_MODEL = \"small\"\n",
        "\n"
      ],
      "metadata": {
        "id": "1aKxmQPe1CKY"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_audio(youtube_url, out_path=\"audio.mp3\"):\n",
        "    ydl_opts = {\"format\": \"bestaudio/best\", \"outtmpl\": out_path}\n",
        "    # download audio\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([youtube_url])\n",
        "        info = ydl.extract_info(youtube_url, download=False)\n",
        "        title = info.get(\"title\", None)\n",
        "    return out_path ,title"
      ],
      "metadata": {
        "id": "JLoSy6o8hySn"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_video_id(url: str):\n",
        "    # Extract video ID\n",
        "    parsed = urlparse(url)\n",
        "    if \"youtu.be\" in parsed.hostname:\n",
        "        video_id = parsed.path[1:]\n",
        "    elif \"watch\" in parsed.path:\n",
        "        video_id = parse_qs(parsed.query)[\"v\"][0]\n",
        "    elif parsed.path.startswith(\"/shorts/\") or parsed.path.startswith(\"/embed/\"):\n",
        "        video_id = parsed.path.split(\"/\")[2]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported YouTube URL format.\")\n",
        "    return video_id"
      ],
      "metadata": {
        "id": "bl-gmvzWdJdG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transcribe\n",
        "def transcribe_whisper(audio_path):\n",
        "    model = whisper.load_model(WHISPER_MODEL)\n",
        "    result = model.transcribe(audio_path, task=\"transcribe\")  # returns segments with timestamps\n",
        "    return result"
      ],
      "metadata": {
        "id": "1MN2vOmG_opL"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# chunking with overlap\n",
        "def chunk_segments(segments, max_chars=1000, overlap_chars=200):\n",
        "    chunks = []\n",
        "    buffer = \"\"\n",
        "    buffer_start = None\n",
        "    buffer_end = None\n",
        "    for seg in segments:\n",
        "        text = seg[\"text\"].strip()\n",
        "        if not buffer:\n",
        "            buffer_start = seg[\"start\"]\n",
        "        if len(buffer) + len(text) <= max_chars:\n",
        "            buffer += (\" \" + text)\n",
        "            buffer_end = seg[\"end\"]\n",
        "        else:\n",
        "            chunks.append({\n",
        "                \"start\": buffer_start, \"end\": buffer_end, \"text\": buffer.strip()\n",
        "            })\n",
        "            # start new buffer with overlap\n",
        "            buffer = text[-overlap_chars:]\n",
        "            buffer_start = seg[\"start\"]\n",
        "            buffer_end = seg[\"end\"]\n",
        "    if buffer:\n",
        "        chunks.append({\"start\": buffer_start, \"end\": buffer_end, \"text\": buffer.strip()})\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "9EqYKKWhE752"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"PINECONE_KEY\"] = userdata.get(\"PINECONE_KEY\")\n"
      ],
      "metadata": {
        "id": "LdftGof3HDED"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_KEY\"])\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ],
      "metadata": {
        "id": "3FjQCQj2G4Pw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings (sentence-transformers)\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "def embed_texts(texts):\n",
        "    return embedder.encode(texts, show_progress_bar=False).tolist()"
      ],
      "metadata": {
        "id": "tjUKu2jsiArg"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dMC3vTW0mLL",
        "outputId": "819d25e5-3bf3-4906-95f8-89ab27fc8dad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
              "                                    'content-length': '183',\n",
              "                                    'content-type': 'application/json',\n",
              "                                    'date': 'Wed, 26 Nov 2025 15:15:59 GMT',\n",
              "                                    'grpc-status': '0',\n",
              "                                    'server': 'envoy',\n",
              "                                    'x-envoy-upstream-service-time': '36',\n",
              "                                    'x-pinecone-request-id': '4920664976898930262',\n",
              "                                    'x-pinecone-request-latency-ms': '35'}},\n",
              " 'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'memoryFullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'__default__': {'vector_count': 12}},\n",
              " 'storageFullness': 0.0,\n",
              " 'total_vector_count': 12,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "index_name = \"youtube-text-demo\"\n",
        "\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=384,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def upsert_chunks(video_id, title, chunks):\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embeddings = embed_texts(texts)  # this should be list of lists\n",
        "    if hasattr(embeddings, \"tolist\"):\n",
        "            embeddings = embeddings.tolist()\n",
        "    vectors = []\n",
        "    for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
        "        # ensure emb is a plain Python list\n",
        "        if not isinstance(emb, list):\n",
        "            emb = emb.tolist()\n",
        "\n",
        "        # ensure metadata contains only serializable types\n",
        "        metadata = {\n",
        "            \"video_id\": video_id,\n",
        "            \"start_time\": float(chunk[\"start\"]),\n",
        "            \"end_time\": float(chunk[\"end\"]),\n",
        "            \"text\": str(chunk[\"text\"]),\n",
        "            \"title\": str(title)\n",
        "        }\n",
        "\n",
        "        vectors.append(\n",
        "            (f\"{video_id}_chunk_{i}\", emb, metadata)\n",
        "        )\n",
        "\n",
        "    # upsert all vectors\n",
        "    index.upsert(vectors=vectors)\n",
        "    print(f\"Upserted {len(vectors)} chunks for video {video_id}\")\n"
      ],
      "metadata": {
        "id": "hgpyWyI2J0jj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def ingest_youtube_video(url):\n",
        "#     print(\"Downloading audio...\")\n",
        "#     audio_path,title = download_audio(url)\n",
        "#     transcript = transcribe_whisper(audio_path)\n",
        "#     chunks=chunk_segments(transcript[\"segments\"])\n",
        "#     video_id=get_video_id(url)\n",
        "#     upsert_chunks(video_id,title,chunks)\n",
        "#     return f\"Successfully ingested video: {url}. Chunks: {len(chunks)}\""
      ],
      "metadata": {
        "id": "ZzotZ26I_031"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def ingest_youtube_video(url: str) -> str:\n",
        "    \"\"\"Ingest a YouTube video by downloading audio, transcribing, and storing chunks.\n",
        "\n",
        "    Args:\n",
        "        url: The YouTube video URL to ingest\n",
        "\n",
        "    Returns:\n",
        "        Success message with number of chunks ingested\n",
        "    \"\"\"\n",
        "    print(\"Downloading audio...\")\n",
        "    audio_path, title = download_audio(url)\n",
        "    transcript = transcribe_whisper(audio_path)\n",
        "    chunks = chunk_segments(transcript[\"segments\"])\n",
        "    video_id = get_video_id(url)\n",
        "    upsert_chunks(video_id, title, chunks)\n",
        "    return f\"Successfully ingested video: {url}. Chunks: {len(chunks)}\""
      ],
      "metadata": {
        "id": "oaqmhPifthHI"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pRZHPgBRGfS",
        "outputId": "74dc3316-1a49-4d24-fdc6-090a0c9b3348"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 1.0.8\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langgraph, pydantic\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def search_vector_db(query: str):\n",
        "#        # 1) embed query\n",
        "#       q_emb = embedder.encode([query])[0]\n",
        "#       if hasattr(q_emb, \"tolist\"):\n",
        "#           q_emb = q_emb.tolist()\n",
        "#       # 2) search vector DB\n",
        "#       results = index.query(\n",
        "#         vector=q_emb,\n",
        "#         top_k=6,             # number of nearest neighbors\n",
        "#         include_metadata=True\n",
        "#       )\n",
        "#       return results"
      ],
      "metadata": {
        "id": "eTGdVAOMjy5C"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_vector_db(query: str) -> str:\n",
        "    \"\"\"Search the vector database for documents similar to the query.\n",
        "\n",
        "    Args:\n",
        "        query: The search query to find similar documents\n",
        "\n",
        "    Returns:\n",
        "        Search results with metadata\n",
        "    \"\"\"\n",
        "    # 1) embed query\n",
        "    q_emb = embedder.encode([query])[0]\n",
        "    if hasattr(q_emb, \"tolist\"):\n",
        "        q_emb = q_emb.tolist()\n",
        "\n",
        "    # 2) search vector DB\n",
        "    results = index.query(\n",
        "        vector=q_emb,\n",
        "        top_k=6,             # 6 nearest neighbors\n",
        "        include_metadata=True\n",
        "    )\n",
        "   # 3) build context\n",
        "    context = \"\"\n",
        "    for r in results[\"matches\"]:\n",
        "        md = r[\"metadata\"]\n",
        "        context += f\"[{md['start_time']:.1f}s - {md['end_time']:.1f}s] {md['text']}\\n\\n\"\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "x6uWBRMzjiPw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_search_agent(user_query, conversation_id=None):\n",
        "#     results=search_vector_db(user_query)\n",
        "\n",
        "\n",
        "\n",
        "#     # 4) system + user prompt\n",
        "#     system_prompt = \"You are an assistant that answers queries using ONLY the provided video excerpts\"\n",
        "#     # query = f\"{system_prompt}\\n\\nContext:\\n{context}\\n\\nQuestion: {user_query}\\nAnswer\"\n",
        "#     # 5) call LLM (could be OpenAI or local)\n",
        "\n",
        "#     llm = OpenAI(model=\"gpt-3.5-turbo-instruct\",temperature = 0.0,openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "\n",
        "#     agent = create_agent(\n",
        "#     model=llm,\n",
        "#     tools=[search_vector_db],\n",
        "#     system_prompt=\"You are a document search assistant. Use the vector database to find relevant documents.\"\n",
        "#     )\n",
        "#     return agent\n",
        "\n"
      ],
      "metadata": {
        "id": "XqxolQ9cO1kg"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "import uuid\n",
        "\n",
        "def answer_query(user_query):\n",
        "    \"\"\"Create and return a configured search agent.\"\"\"\n",
        "\n",
        "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\",openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "    checkpointer = MemorySaver()\n",
        "\n",
        "\n",
        "    agent = create_agent(\n",
        "        model=llm,\n",
        "        tools=[ingest_youtube_video,search_vector_db],\n",
        "         system_prompt=\"\"\"You are a video knowledge management assistant.\n",
        "          You can:\n",
        "          1. Ingest YouTube videos into the knowledge base\n",
        "          2. Search for relevant information in previously ingested videos\n",
        "          When a user provides a YouTube URL, ingest it. When they ask questions, search the knowledge base.\"\"\",\n",
        "        checkpointer=checkpointer  # Enable memory\n",
        "    )\n",
        "    thread_id = str(uuid.uuid4())\n",
        "    result=agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\":user_query }]\n",
        "   },\n",
        "   config={\"configurable\": {\"thread_id\": thread_id}} )\n",
        "    return result"
      ],
      "metadata": {
        "id": "BEYtz0QTkaas"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain MCP in simple terms.\"\n",
        "result = answer_query(question)\n",
        "answer = result[\"messages\"][-1].content\n",
        "print(\"Final Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCy094R3PVFN",
        "outputId": "86b84b93-5a50-4a77-d113-ab341cd6f368"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "MCP stands for Model Context Protocol. It is a protocol that enables AI agents to dynamically and autonomously interact with software systems. Unlike traditional APIs, which require manual integration by human developers, MCP allows AI agents to access a server that provides a machine-readable menu of its capabilities. This means the AI agent can understand and use the available tools without the need for manual coding. MCP complements APIs by serving as a smart universal remote control for AI agents to operate software systems effectively. While APIs are like a restaurant menu where human developers place specific orders, MCP allows AI agents to make intelligent decisions based on the available tools without explicit coding for every action.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"https://www.youtube.com/watch?v=LPZh9BOjkQs\"\n",
        "# result = answer_query(question)\n",
        "# answer = result[\"messages\"][-1].content\n",
        "# print(\"Final Answer:\")\n",
        "# print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN4NpvfIge5_",
        "outputId": "e13c3cc7-250f-4fd0-c8f7-939e2efb204a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading audio...\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=LPZh9BOjkQs\n",
            "[youtube] LPZh9BOjkQs: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] LPZh9BOjkQs: Downloading android sdkless player API JSON\n",
            "[youtube] LPZh9BOjkQs: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] LPZh9BOjkQs: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] LPZh9BOjkQs: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] LPZh9BOjkQs: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] LPZh9BOjkQs: Downloading 1 format(s): 251-11\n",
            "[download] audio.mp3 has already been downloaded\n",
            "[download] 100% of    7.79MiB\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=LPZh9BOjkQs\n",
            "[youtube] LPZh9BOjkQs: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] LPZh9BOjkQs: Downloading android sdkless player API JSON\n",
            "[youtube] LPZh9BOjkQs: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] LPZh9BOjkQs: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] LPZh9BOjkQs: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] LPZh9BOjkQs: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 8 chunks for video LPZh9BOjkQs\n",
            "Final Answer:\n",
            "The YouTube video has been successfully ingested, and it has been divided into 8 chunks for analysis. What would you like to know about this video?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is back prapogation\"\n",
        "result = answer_query(question)\n",
        "answer = result[\"messages\"][-1].content\n",
        "print(\"Final Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5reVY69x_nQ",
        "outputId": "7966fe13-1b45-42c9-901b-9c13eaa08a26"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Backpropagation is an algorithm used to tweak all the parameters in a language model during training. The parameters in a language model are continuously refined based on many example pieces of text. Backpropagation is used to adjust these parameters based on the predictions made by the model compared to the true values in the training data. This process helps the model learn and improve its accuracy in making predictions.\n"
          ]
        }
      ]
    }
  ]
}