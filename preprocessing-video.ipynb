{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s11khushboo/youtube-QandA/blob/main/preprocessing-video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp openai-whisper sentence-transformers pinecone\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GGmSbW__1Mpn",
        "outputId": "f8322ecc-763b-4d3e-bbdc-2b7ee66fdf45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.11.12-py3-none-any.whl.metadata (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-8.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2025.11.12)\n",
            "Requirement already satisfied: orjson>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.11.4)\n",
            "Collecting pinecone-plugin-assistant<4.0.0,>=3.0.1 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pinecone-plugin-interface<0.1.0,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Collecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Downloading yt_dlp-2025.11.12-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-8.0.0-py3-none-any.whl (745 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m745.9/745.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.9/280.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=1326b55ee33e784d461d12203065b65cc4d9ec7736403b2f7d8dcd8577000c79\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: yt-dlp, pinecone-plugin-interface, packaging, pinecone-plugin-assistant, pinecone, openai-whisper\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "Successfully installed openai-whisper-20250625 packaging-24.2 pinecone-8.0.0 pinecone-plugin-assistant-3.0.1 pinecone-plugin-interface-0.0.7 yt-dlp-2025.11.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "35ad117299354e7cb72e6ac5a1116c82"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ingest.py (simplified)\n",
        "from yt_dlp import YoutubeDL\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pinecone\n",
        "import uuid\n",
        "import math\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate  # pseudo imports\n",
        "\n",
        "\n",
        "\n",
        "INDEX_NAME = \"youtube-chunks\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"  # or OpenAI embeddings\n",
        "WHISPER_MODEL = \"small\"\n",
        "\n"
      ],
      "metadata": {
        "id": "1aKxmQPe1CKY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_audio(youtube_url, out_path=\"audio.mp3\"):\n",
        "    ydl_opts = {\"format\": \"bestaudio/best\", \"outtmpl\": out_path}\n",
        "    # download audio\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([youtube_url])\n",
        "        info = ydl.extract_info(youtube_url, download=False)\n",
        "        title = info.get(\"title\", None)\n",
        "    return out_path ,title"
      ],
      "metadata": {
        "id": "JLoSy6o8hySn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_video_id(url: str):\n",
        "    # Extract video ID\n",
        "    parsed = urlparse(url)\n",
        "    if \"youtu.be\" in parsed.hostname:\n",
        "        video_id = parsed.path[1:]\n",
        "    elif \"watch\" in parsed.path:\n",
        "        video_id = parse_qs(parsed.query)[\"v\"][0]\n",
        "    elif parsed.path.startswith(\"/shorts/\") or parsed.path.startswith(\"/embed/\"):\n",
        "        video_id = parsed.path.split(\"/\")[2]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported YouTube URL format.\")\n",
        "    return video_id"
      ],
      "metadata": {
        "id": "bl-gmvzWdJdG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transcribe\n",
        "def transcribe_whisper(audio_path):\n",
        "    model = whisper.load_model(WHISPER_MODEL)\n",
        "    result = model.transcribe(audio_path, task=\"transcribe\")  # returns segments with timestamps\n",
        "    return result"
      ],
      "metadata": {
        "id": "1MN2vOmG_opL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# chunking with overlap\n",
        "def chunk_segments(segments, max_chars=1000, overlap_chars=200):\n",
        "    chunks = []\n",
        "    buffer = \"\"\n",
        "    buffer_start = None\n",
        "    buffer_end = None\n",
        "    for seg in segments:\n",
        "        text = seg[\"text\"].strip()\n",
        "        if not buffer:\n",
        "            buffer_start = seg[\"start\"]\n",
        "        if len(buffer) + len(text) <= max_chars:\n",
        "            buffer += (\" \" + text)\n",
        "            buffer_end = seg[\"end\"]\n",
        "        else:\n",
        "            chunks.append({\n",
        "                \"start\": buffer_start, \"end\": buffer_end, \"text\": buffer.strip()\n",
        "            })\n",
        "            # start new buffer with overlap\n",
        "            buffer = text[-overlap_chars:]\n",
        "            buffer_start = seg[\"start\"]\n",
        "            buffer_end = seg[\"end\"]\n",
        "    if buffer:\n",
        "        chunks.append({\"start\": buffer_start, \"end\": buffer_end, \"text\": buffer.strip()})\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "9EqYKKWhE752"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"PINECONE_KEY\"] = userdata.get(\"PINECONE_KEY\")\n"
      ],
      "metadata": {
        "id": "LdftGof3HDED"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_KEY\"])\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ],
      "metadata": {
        "id": "3FjQCQj2G4Pw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dMC3vTW0mLL",
        "outputId": "cfb8c71c-ba5c-41e8-b75c-9cb67e9fb914"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
              "                                    'content-length': '181',\n",
              "                                    'content-type': 'application/json',\n",
              "                                    'date': 'Wed, 26 Nov 2025 09:23:51 GMT',\n",
              "                                    'grpc-status': '0',\n",
              "                                    'server': 'envoy',\n",
              "                                    'x-envoy-upstream-service-time': '52',\n",
              "                                    'x-pinecone-request-id': '7836427583778180564',\n",
              "                                    'x-pinecone-request-latency-ms': '51'}},\n",
              " 'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'memoryFullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'__default__': {'vector_count': 8}},\n",
              " 'storageFullness': 0.0,\n",
              " 'total_vector_count': 8,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# embeddings (sentence-transformers)\n",
        "embedder = SentenceTransformer(EMBED_MODEL)\n",
        "\n",
        "def embed_texts(texts):\n",
        "    return embedder.encode(texts, show_progress_bar=False).tolist()\n",
        "\n",
        "\n",
        "index_name = \"youtube-text-demo\"\n",
        "\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=384,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def upsert_chunks(video_id, title, chunks):\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embeddings = embed_texts(texts)  # this should be list of lists\n",
        "    if hasattr(embeddings, \"tolist\"):\n",
        "            embeddings = embeddings.tolist()\n",
        "    vectors = []\n",
        "    for i, (chunk, emb) in enumerate(zip(chunks, embeddings)):\n",
        "        # ensure emb is a plain Python list\n",
        "        if not isinstance(emb, list):\n",
        "            emb = emb.tolist()\n",
        "\n",
        "        # ensure metadata contains only serializable types\n",
        "        metadata = {\n",
        "            \"video_id\": video_id,\n",
        "            \"start_time\": float(chunk[\"start\"]),\n",
        "            \"end_time\": float(chunk[\"end\"]),\n",
        "            \"text\": str(chunk[\"text\"]),\n",
        "            \"title\": str(title)\n",
        "        }\n",
        "\n",
        "        vectors.append(\n",
        "            (f\"{video_id}_chunk_{i}\", emb, metadata)\n",
        "        )\n",
        "\n",
        "    # upsert all vectors\n",
        "    index.upsert(vectors=vectors)\n",
        "    print(f\"Upserted {len(vectors)} chunks for video {video_id}\")\n"
      ],
      "metadata": {
        "id": "hgpyWyI2J0jj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_youtube_video(url):\n",
        "    print(\"Downloading audio...\")\n",
        "    audio_path,title = download_audio(url)\n",
        "    transcript = transcribe_whisper(audio_path)\n",
        "    chunks=chunk_segments(transcript[\"segments\"])\n",
        "    video_id=get_video_id(url)\n",
        "    upsert_chunks(video_id,title,chunks)\n",
        "    return f\"Successfully ingested video: {url}. Chunks: {len(chunks)}\""
      ],
      "metadata": {
        "id": "ZzotZ26I_031"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ingest_youtube_video(\"https://www.youtube.com/watch?v=67_aMPDk2zw\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "n5_SkEQeeyKb",
        "outputId": "3630c34f-73fc-457c-92b0-3231f85e8ffa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading audio...\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=67_aMPDk2zw\n",
            "[youtube] 67_aMPDk2zw: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] 67_aMPDk2zw: Downloading android sdkless player API JSON\n",
            "[youtube] 67_aMPDk2zw: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] 67_aMPDk2zw: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] 67_aMPDk2zw: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] 67_aMPDk2zw: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] 67_aMPDk2zw: Downloading 1 format(s): 251-9\n",
            "[download] audio.mp3 has already been downloaded\n",
            "[download] 100% of    4.35MiB\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=67_aMPDk2zw\n",
            "[youtube] 67_aMPDk2zw: Downloading webpage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one. To silence this warning, you can use  --extractor-args \"youtube:player_client=default\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] 67_aMPDk2zw: Downloading android sdkless player API JSON\n",
            "[youtube] 67_aMPDk2zw: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] 67_aMPDk2zw: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] 67_aMPDk2zw: Downloading m3u8 information\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] 67_aMPDk2zw: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "100%|███████████████████████████████████████| 461M/461M [00:06<00:00, 69.9MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 4 chunks for video 67_aMPDk2zw\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Successfully ingested video: https://www.youtube.com/watch?v=67_aMPDk2zw. Chunks: 4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pRZHPgBRGfS",
        "outputId": "4f1b298a-7137-4ac6-e3df-272b92a6a360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 1.0.8\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://docs.langchain.com/\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, langgraph, pydantic\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apggDQhiR25o",
        "outputId": "9e581189-6f86-4ad4-9632-4115f40d059a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.1.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.8.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_vector_db(query: str):\n",
        "       # 1) embed query\n",
        "      q_emb = embedder.encode([query])[0]\n",
        "      if hasattr(q_emb, \"tolist\"):\n",
        "          q_emb = q_emb.tolist()\n",
        "      # 2) search vector DB\n",
        "      results = index.query(\n",
        "        vector=q_emb,\n",
        "        top_k=6,             # number of nearest neighbors\n",
        "        include_metadata=True\n",
        "      )\n",
        "      return results"
      ],
      "metadata": {
        "id": "eTGdVAOMjy5C"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query(user_query, conversation_id=None):\n",
        "    results=search_vector_db(user_query)\n",
        "\n",
        "\n",
        "    # 3) build context\n",
        "    context = \"\"\n",
        "    for r in results[\"matches\"]:\n",
        "        md = r[\"metadata\"]\n",
        "        context += f\"[{md['start_time']:.1f}s - {md['end_time']:.1f}s] {md['text']}\\n\\n\"\n",
        "    # 4) system + user prompt\n",
        "    system_prompt = \"You are an assistant that answers queries using ONLY the provided video excerpts\"\n",
        "    query = f\"{system_prompt}\\n\\nContext:\\n{context}\\n\\nQuestion: {user_query}\\nAnswer\"\n",
        "    # 5) call LLM (could be OpenAI or local)\n",
        "\n",
        "    llm = llm = OpenAI(model=\"gpt-3.5-turbo-instruct\",temperature = 0.0,openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    answer = llm.invoke(query)\n",
        "    print(answer)\n",
        "    # 6) store memory (optional)\n",
        "    # ... save conversation and retrieved ids\n",
        "    return {\"answer\": answer, \"sources\": [m['id'] for m in results['matches']]}\n"
      ],
      "metadata": {
        "id": "XqxolQ9cO1kg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain MCP in simple terms.\"\n",
        "answer = answer_query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCy094R3PVFN",
        "outputId": "4ffad39e-7df1-40b6-b6d9-2efff4ec8728"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results\n",
            ": MCP, or Model Context Protocol, is a way for AI agents to interact with software and tools without the need for human developers to write explicit code for every action. It allows AI agents to discover and use tools on their own, making the process faster and more efficient. MCP is different from traditional APIs, which require human developers to manually integrate software systems. Instead, MCP provides a machine-readable menu of its capabilities, allowing AI agents to dynamically and autonomously complete tasks without the need for external documentation or pre-written code. MCP will complement APIs, not replace them, and will work together with them to make systems more resilient and adaptable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is LLM\"\n",
        "answer = answer_query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E11QZOJCgJ9T",
        "outputId": "21301e59-4fdb-4a00-ddef-99232f22efd4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results\n",
            ": LLM stands for Large Language Model. It is a type of language model that is trained on a large volume of data, such as Wikipedia articles, Google news articles, and online books. LLMs use a neural network with trillions of parameters to capture complex patterns and nuances in language. They are used in applications such as Chat GPT and Gmail auto-complete. LLMs also use reinforcement learning with human feedback to improve their performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is stochastic parrot\"\n",
        "answer = answer_query(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN4NpvfIge5_",
        "outputId": "5c9824e6-d5bf-4b0c-b56f-5ad40f2820d5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": A stochastic parrot is a parrot that uses statistical probability and randomness to predict the next word or set of words based on past conversations it has listened to. It is similar to a language model, which uses neural networks to predict the next set of words in a sentence.\n",
            "{'answer': ': A stochastic parrot is a parrot that uses statistical probability and randomness to predict the next word or set of words based on past conversations it has listened to. It is similar to a language model, which uses neural networks to predict the next set of words in a sentence.', 'sources': ['67_aMPDk2zw_chunk_0', '67_aMPDk2zw_chunk_2', '67_aMPDk2zw_chunk_3', 'dwlE7TiDXz40_chunk_6', 'dwlE7TiDXz40_chunk_0', '67_aMPDk2zw_chunk_1']}\n"
          ]
        }
      ]
    }
  ]
}